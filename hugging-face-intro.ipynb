{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-04T07:30:01.023312Z",
     "start_time": "2026-02-04T07:26:23.201543Z"
    }
   },
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "!pip install -U transformers torch"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "  Downloading transformers-5.0.0-py3-none-any.whl.metadata (37 kB)\r\n",
      "Collecting torch\r\n",
      "  Downloading torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl.metadata (25 kB)\r\n",
      "Collecting filelock (from transformers)\r\n",
      "  Downloading filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\r\n",
      "  Downloading huggingface_hub-1.3.7-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting numpy>=1.17 (from transformers)\r\n",
      "  Downloading numpy-2.2.6-cp310-cp310-macosx_14_0_x86_64.whl.metadata (62 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (26.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.3)\r\n",
      "Collecting regex!=2019.12.17 (from transformers)\r\n",
      "  Downloading regex-2026.1.15-cp310-cp310-macosx_10_9_x86_64.whl.metadata (40 kB)\r\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\r\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-macosx_10_12_x86_64.whl.metadata (7.3 kB)\r\n",
      "Collecting typer-slim (from transformers)\r\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\r\n",
      "Collecting safetensors>=0.4.3 (from transformers)\r\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_10_12_x86_64.whl.metadata (4.1 kB)\r\n",
      "Collecting tqdm>=4.27 (from transformers)\r\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.15.0)\r\n",
      "Collecting sympy (from torch)\r\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting networkx (from torch)\r\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.6)\r\n",
      "Collecting fsspec (from torch)\r\n",
      "  Downloading fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\r\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.9 kB)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\r\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=1.3.0->transformers)\r\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\r\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\r\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Collecting click>=8.0.0 (from typer-slim->transformers)\r\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\r\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\r\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\r\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.1)\r\n",
      "Downloading transformers-5.0.0-py3-none-any.whl (10.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m0m\r\n",
      "\u001B[?25hDownloading torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl (150.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m150.8/150.8 MB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:04\u001B[0m\r\n",
      "\u001B[?25hDownloading huggingface_hub-1.3.7-py3-none-any.whl (536 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.7/536.7 kB\u001B[0m \u001B[31m498.6 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hDownloading fsspec-2026.1.0-py3-none-any.whl (201 kB)\r\n",
      "Downloading numpy-2.2.6-cp310-cp310-macosx_14_0_x86_64.whl (6.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.9/6.9 MB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading regex-2026.1.15-cp310-cp310-macosx_10_9_x86_64.whl (290 kB)\r\n",
      "Downloading safetensors-0.7.0-cp38-abi3-macosx_10_12_x86_64.whl (467 kB)\r\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-macosx_10_12_x86_64.whl (3.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading tqdm-4.67.3-py3-none-any.whl (78 kB)\r\n",
      "Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\r\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m0m\r\n",
      "\u001B[?25hDownloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\r\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\r\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl (2.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.9/2.9 MB\u001B[0m \u001B[31m1.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m-:--:--\u001B[0m\r\n",
      "\u001B[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\r\n",
      "Installing collected packages: mpmath, tqdm, sympy, shellingham, safetensors, regex, numpy, networkx, hf-xet, fsspec, filelock, click, typer-slim, torch, huggingface-hub, tokenizers, transformers\r\n",
      "Successfully installed click-8.3.1 filelock-3.20.3 fsspec-2026.1.0 hf-xet-1.2.0 huggingface-hub-1.3.7 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 regex-2026.1.15 safetensors-0.7.0 shellingham-1.5.4 sympy-1.14.0 tokenizers-0.22.2 torch-2.2.2 tqdm-4.67.3 transformers-5.0.0 typer-slim-0.21.1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:31:42.744332Z",
     "start_time": "2026-02-04T10:31:19.807265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ],
   "id": "75801ac9f1c3414a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/PycharmProjects/inference-with-huggingface/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:31:44.664744Z",
     "start_time": "2026-02-04T10:31:42.752402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Load pretrained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ],
   "id": "bcacdfee854e05f4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:31:44.675728Z",
     "start_time": "2026-02-04T10:31:44.669701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"fuck you,\"\n",
    "indexed_tokens = tokenizer.encode(text)"
   ],
   "id": "cbd23315823da94e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:31:44.685999Z",
     "start_time": "2026-02-04T10:31:44.680230Z"
    }
   },
   "cell_type": "code",
   "source": "token_tensor = torch.tensor([indexed_tokens])",
   "id": "4864dd6a6828c63e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:31:44.695735Z",
     "start_time": "2026-02-04T10:31:44.692757Z"
    }
   },
   "cell_type": "code",
   "source": "print(indexed_tokens)",
   "id": "5ab16b5ae6d8aaeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31699, 345, 11]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:31:45.944037Z",
     "start_time": "2026-02-04T10:31:44.718657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Load pretrained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ],
   "id": "c33d1c58f79c23a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 537.56it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:31:46.637021Z",
     "start_time": "2026-02-04T10:31:45.950439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#No backpropagation\n",
    "with torch.no_grad():\n",
    "    outputs = model(token_tensor)\n",
    "    predictions = outputs[0]\n"
   ],
   "id": "c9d1d982b713c97",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:31:46.649600Z",
     "start_time": "2026-02-04T10:31:46.644528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Get the predict next subword\n",
    "predict_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "predict_text = tokenizer.decode(indexed_tokens + [predict_index])\n",
    "print(predict_text)"
   ],
   "id": "4f1bdecb7f547799",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck you, I\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:31:49.030875Z",
     "start_time": "2026-02-04T10:31:46.665580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Your prompt\n",
    "prompt = \"I don't know, what should I do\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate continuation (sentence)\n",
    "# max_new_tokens controls how long the generated text will be\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,   # adjust for longer sentence\n",
    "    do_sample=True,      # enables randomness (otherwise argmax)\n",
    "    top_k=50,            # sample from top 50 tokens\n",
    "    top_p=0.98,          # nucleus sampling\n",
    "    temperature=0.8      # randomness factor\n",
    ")\n",
    "\n",
    "# Decode generated text\n",
    "generated_text = tokenizer.decode(output_ids[0])\n",
    "print(generated_text)\n"
   ],
   "id": "d19ed4c2357492aa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know, what should I do. I need to get a job.\"\n",
      "\n",
      "\"Okay, you have to get a job, and I'm going to need to get a job for my son.\"\n",
      "\n",
      "\"I'm good with kids, if I want to be with my\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "86a42e712712cb66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
